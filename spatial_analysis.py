# -*- coding: utf-8 -*-
"""spatial_analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HbQ8EUOsC3w8KBlzCS7hhHYndfWqQbS8
"""

import streamlit as st
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
from sklearn.preprocessing import StandardScaler
import plotly.express as px
import io

# PySAL and spopt imports
import libpysal
from spopt.region import WardSpatial

# --- Streamlit Page Configuration ---
st.set_page_config(layout="wide", page_title="HCP Geographic Regionalization (WardSpatial)")

st.title("HCP Geographic Regionalization Tool (Location-Focused WardSpatial)")
st.markdown("""
This tool uses the **WardSpatial algorithm** to create geographically coherent national regions.
It **prioritizes geographic location (projected X, Y coordinates)** for forming these broad regions.
Transaction count (`trx_count`) is then a characteristic of these regions.

**Instructions:**
1.  Upload a CSV file with columns: `hcp_id`, `trx_count`, `latitude`, `longitude`.
    *   *(Optional):* Include `state`, `city`, `zip_code` for validation.
2.  Select the desired number of **broad national regions**.
3.  Adjust the **Number of Neighbors (KNN)** for connectivity (use a very small value like 2-4 for national data).
4.  Click 'Run Regionalization'.
""")

# --- File Upload ---
uploaded_file = st.file_uploader("1. Upload your HCP Data (CSV)", type="csv")

if uploaded_file is not None:
    try:
        df = pd.read_csv(uploaded_file)
        st.success("File Uploaded Successfully!")

        # --- Data Validation ---
        required_columns = ['hcp_id', 'trx_count', 'latitude', 'longitude']
        optional_geo_columns = ['state', 'city', 'zip_code']
        present_optional_geo = [col for col in optional_geo_columns if col in df.columns]

        if not all(col in df.columns for col in required_columns):
            st.error(f"Error: CSV must contain the core columns: {', '.join(required_columns)}")
            st.stop()

        try:
            df['trx_count'] = pd.to_numeric(df['trx_count'])
            df['latitude'] = pd.to_numeric(df['latitude'])
            df['longitude'] = pd.to_numeric(df['longitude'])
        except ValueError as e:
            st.error(f"Error converting data to numeric types. Please check 'trx_count', 'latitude', 'longitude'. Details: {e}")
            st.stop()

        st.write("### Input Data Preview (First 5 Rows)")
        st.dataframe(df.head())

        initial_rows = len(df)
        df_cleaned = df.dropna(subset=['trx_count', 'latitude', 'longitude']).copy()
        rows_dropped = initial_rows - len(df_cleaned)
        if rows_dropped > 0:
            st.warning(f"Warning: Dropped {rows_dropped} rows due to missing values in core columns.")

        if len(df_cleaned) < 5: # WardSpatial needs a few points
             st.error("Error: Not enough valid data (minimum ~5 recommended for this algorithm).")
             st.stop()

        # --- Convert to GeoDataFrame and Project ---
        st.write("DEBUG: Converting to GeoDataFrame and projecting coordinates...")
        geometry = [Point(xy) for xy in zip(df_cleaned['longitude'], df_cleaned['latitude'])]
        gdf = gpd.GeoDataFrame(df_cleaned, geometry=geometry, crs="EPSG:4326") # Assume WGS84
        # EPSG:5070 is NAD83 / Conus Albers - good for contiguous US
        # For broader North America including AK, consider EPSG:3338 (Alaska Albers) or a custom Lambert Azimuthal Equal Area
        # For simplicity, we'll stick to EPSG:5070 for now.
        gdf_projected = gdf.to_crs("EPSG:5070")
        st.write(f"DEBUG: Data projected to EPSG:5070.")

        # Add projected coordinates as columns, these will be scaled and used for clustering
        gdf_projected['proj_x'] = gdf_projected.geometry.x
        gdf_projected['proj_y'] = gdf_projected.geometry.y

        # --- User Input for WardSpatial Parameters ---
        st.sidebar.header("Regionalization Parameters")
        n_regions = st.sidebar.slider("2. Number of National Regions:",
                                          min_value=2,
                                          # Max regions: suggest fewer for broad national
                                          max_value=min(10, len(gdf_projected)//3 if len(gdf_projected)//3 >=2 else 2),
                                          value=min(3, len(gdf_projected)//3 if len(gdf_projected)//3 >=2 else 2), # Default to 3-4 broad regions
                                          step=1,
                                          help="Select the number of broad geographic regions (e.g., West, Central, East).")

        max_k_neighbors = len(gdf_projected) - 1
        if max_k_neighbors < 1: # Should be caught by len(df_cleaned) < 5 but good to have
            st.error("Not enough unique data points to define neighbors.")
            st.stop()

        # CRITICAL: Use a very small k_neighbors for national data to enforce local connectivity
        k_neighbors_val = st.sidebar.slider("Number of Neighbors (KNN for connectivity):",
                                        min_value=2, # Minimum 2 for some stability in graph
                                        max_value=min(5, max_k_neighbors), # Max 5 for national data to keep it local
                                        value=min(3, max_k_neighbors), # Default to a very small k
                                        step=1,
                                        help="CRITICAL: Use a very small value (e.g., 2-4) for national data to ensure local connections only.")


        # --- WardSpatial Execution ---
        st.markdown("---")
        if st.button(f"3. Run Geographic Regionalization for {n_regions} Regions", type="primary"):
            with st.spinner(f'Building KNN (k={k_neighbors_val}) graph and running WardSpatial... This may take a moment.'):

                # --- Prepare attributes for WardSpatial: ONLY scaled projected coordinates ---
                attributes_to_cluster_on = ['proj_x', 'proj_y']
                data_for_coord_scaling = gdf_projected[attributes_to_cluster_on].copy()

                # Scale these coordinates
                coord_scaler = StandardScaler()
                scaled_coordinates = coord_scaler.fit_transform(data_for_coord_scaling)

                # Add scaled coordinates to a temporary GeoDataFrame for WardSpatial
                gdf_for_model = gdf_projected.copy() # Start with a fresh copy
                gdf_for_model['scaled_proj_x'] = scaled_coordinates[:, 0]
                gdf_for_model['scaled_proj_y'] = scaled_coordinates[:, 1]

                # These are the attributes WardSpatial will use to minimize variance
                attribute_names_for_model = ['scaled_proj_x', 'scaled_proj_y']
                st.write(f"DEBUG: Attributes for WardSpatial (scaled geographic coordinates only): {attribute_names_for_model}")

                # Create KNN spatial weights matrix from projected coordinates
                st.write(f"DEBUG: Building KNN weights matrix with k={k_neighbors_val}...")
                try:
                    # Ensure active geometry is set for libpysal
                    gdf_projected_for_weights = gdf_projected.set_geometry('geometry')
                    knn_weights = libpysal.weights.KNN.from_dataframe(gdf_projected_for_weights, k=k_neighbors_val)
                    st.write("DEBUG: KNN weights matrix built.")
                except Exception as e_weights:
                    st.error(f"Error building spatial weights: {e_weights}")
                    st.error("This can happen if k_neighbors is too small/large for the data distribution, or if there are many coincident points.")
                    st.stop()

                # --- Run WardSpatial ---
                st.write("DEBUG: Running WardSpatial algorithm...")
                try:
                    model_ward = WardSpatial(
                        gdf_for_model, # GeoDataFrame with scaled coordinate columns
                        w=knn_weights,
                        attrs_name=attribute_names_for_model, # Use ONLY scaled coordinates
                        n_clusters=n_regions
                    )
                    model_ward.solve()
                    st.write("DEBUG: WardSpatial solved.")
                except Exception as e_ward:
                    st.error(f"Error during WardSpatial execution: {e_ward}")
                    st.error("This might be due to issues with the weights matrix (e.g., disconnected graph for the chosen k_neighbors) or the number of clusters relative to data points.")
                    st.stop()


                # Assign cluster labels back to the original GeoDataFrame (gdf)
                # model_ward.labels_ corresponds to the order in gdf_for_model (which is same as gdf_projected)
                gdf.loc[gdf_for_model.index, 'region_id'] = model_ward.labels_

            st.success(f"Geographic Regionalization Complete! {n_regions} regions generated.")
            st.markdown("---")

            # --- Display Results ---
            st.write("### 4. Regionalization Results")

            # --- Map Visualization ---
            st.write("#### Interactive Map of Geographic Regions")
            st.markdown("HCP locations colored by assigned region. Hover for details.")
            try:
                gdf['region_id'] = gdf['region_id'].astype(str) # For discrete colors
                hover_data_dict = {"latitude": False, "longitude": False, "region_id": True, "trx_count": True}
                for col in present_optional_geo:
                    if col in gdf.columns: hover_data_dict[col] = True

                fig = px.scatter_mapbox(gdf.dropna(subset=['region_id']),
                                        lat="latitude",
                                        lon="longitude",
                                        color="region_id",
                                        size="trx_count", # Still size by trx_count for visual info
                                        hover_name="hcp_id",
                                        hover_data=hover_data_dict,
                                        color_discrete_sequence=px.colors.qualitative.Set1, # Using a different palette
                                        zoom=3.0, # Zoom out a bit more for national view
                                        height=600,
                                        mapbox_style="carto-positron")
                fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
                st.plotly_chart(fig, use_container_width=True)
            except Exception as map_error:
                st.error(f"Error creating map: {map_error}")

            st.markdown("---")

            # --- Geographic Region Summary ---
            if present_optional_geo:
                st.write("#### Geographic Region Summary")
                st.markdown(f"Count of HCPs per Region and {', '.join(present_optional_geo)}.")
                grouping_fields = ['region_id'] + [col for col in present_optional_geo if col in gdf.columns]
                if len(grouping_fields) > 1 and 'region_id' in gdf.columns:
                    geo_summary_df = gdf.dropna(subset=['region_id'])
                    geo_summary = geo_summary_df.groupby(grouping_fields).size().reset_index(name='HCP Count')
                    st.dataframe(geo_summary.sort_values(by=['region_id'] + [col for col in present_optional_geo if col in gdf.columns]))
                else:
                    st.write("Optional geographic columns or region assignments not found for summary.")
                st.markdown("*(Use this table to check if regions are geographically consistent)*")
                st.markdown("---")

            # --- Results Table ---
            st.write("#### Full Segmented Data Table")
            display_columns = ['hcp_id', 'trx_count', 'latitude', 'longitude'] + \
                              [col for col in present_optional_geo if col in gdf.columns] + \
                              ['region_id']
            final_display_columns = [col for col in display_columns if col in gdf.columns]
            if 'region_id' in gdf.columns:
                st.dataframe(gdf[final_display_columns].sort_values('region_id'))
            else:
                st.write("Region information not available for the table.")


            # --- Download Button ---
            st.markdown("---")
            st.write("### 5. Export Results")
            try:
                output = io.BytesIO()
                if 'region_id' in gdf.columns:
                    df_to_save = gdf[final_display_columns]
                    df_to_save.to_csv(output, index=False, encoding='utf-8')
                    output.seek(0)
                    st.download_button(label="Download Regionalized Data as CSV",
                                   data=output,
                                   file_name=f'hcp_geographic_regions_{n_regions}.csv',
                                   mime='text/csv',
                                   key='download-ward-geo-regions-csv')
                else:
                    st.write("No regionalized data to download.")
            except Exception as download_error:
                st.error(f"Error preparing download link: {download_error}")

    except pd.errors.EmptyDataError:
        st.error("Error: The uploaded CSV file appears to be empty.")
    except ImportError as e_import:
        st.error(f"ImportError: A required library (likely PySAL, spopt, or a dependency) is not installed. Details: {e_import}")
        st.error("Please ensure your environment has all libraries from requirements.txt installed, especially GeoPandas and PySAL components.")
    except Exception as e:
        st.error(f"An unexpected error occurred: {e}")
        st.error("Please ensure the uploaded file is valid and all dependencies are installed.")
        # import traceback # Uncomment for detailed traceback
        # st.code(traceback.format_exc())

else:
    st.info("Awaiting CSV file upload to begin.")